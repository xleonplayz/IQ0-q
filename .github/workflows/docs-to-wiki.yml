name: Publish Documentation to Wiki

on:
  # Nur manuell ausfÃ¼hren, nicht bei Push
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Force update wiki'
        required: false
        default: 'false'

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - name: Checkout source repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install sphinx sphinx-rtd-theme myst-parser

      - name: Install documentation tools
        run: |
          sudo apt-get update
          sudo apt-get install -y doxygen graphviz

      - name: Generate Doxygen documentation
        run: |
          cd docs
          doxygen Doxyfile
          
      - name: Build Sphinx documentation
        run: |
          cd docs
          sphinx-build -b html . _build/html
          echo "Sphinx documentation built successfully"
          
      - name: Convert Doxygen to Markdown
        run: |
          pip install beautifulsoup4 lxml
          cat > convert.py << 'EOF'
          import os
          import sys
          from bs4 import BeautifulSoup
          import re
          import glob
          
          def html_to_markdown(html_file, output_dir):
              with open(html_file, 'r', encoding='utf-8') as f:
                  html_content = f.read()
              
              soup = BeautifulSoup(html_content, 'lxml')
              
              # Extract page title
              title = soup.title.string.replace(" Source File", "").replace(" Class Reference", "")
              
              # Initialize markdown content
              md_content = f"# {title}\n\n"
              
              # Get the main content div
              content_div = soup.find('div', {'class': 'contents'})
              if content_div:
                  # Handle headings
                  for tag in content_div.find_all(['h1', 'h2', 'h3', 'h4', 'h5']):
                      level = int(tag.name[1])
                      tag.replace_with(f"{'#' * level} {tag.text}\n\n")
                  
                  # Handle code blocks
                  for pre in content_div.find_all('pre', {'class': 'fragment'}):
                      language = 'python' if '.py' in html_file else ''
                      code = pre.text.strip()
                      pre.replace_with(f"```{language}\n{code}\n```\n\n")
                  
                  # Handle tables
                  for table in content_div.find_all('table'):
                      md_table = "| "
                      headers = table.find_all('th')
                      if headers:
                          md_table += " | ".join([h.text.strip() for h in headers]) + " |\n"
                          md_table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
                      
                      for row in table.find_all('tr'):
                          cells = row.find_all('td')
                          if cells:
                              md_table += "| " + " | ".join([c.text.strip() for c in cells]) + " |\n"
                      
                      table.replace_with(md_table + "\n")
                  
                  # Handle links
                  for a in content_div.find_all('a'):
                      href = a.get('href', '')
                      if href and not href.startswith('#') and not href.startswith('http'):
                          # Convert local .html links to .md
                          href = href.replace('.html', '.md')
                      a.replace_with(f"[{a.text}]({href})")
                  
                  # Convert remaining content
                  md_content += content_div.text
              
              # Clean up the markdown
              md_content = re.sub(r'\n{3,}', '\n\n', md_content)
              
              # Determine output filename
              filename = os.path.basename(html_file).replace('.html', '.md')
              output_path = os.path.join(output_dir, filename)
              
              with open(output_path, 'w', encoding='utf-8') as f:
                  f.write(md_content)
              
              return output_path
          
          def main():
              input_dir = 'docs_output/html'
              output_dir = 'wiki'
              
              if not os.path.exists(output_dir):
                  os.makedirs(output_dir)
              
              # Process all HTML files
              html_files = glob.glob(os.path.join(input_dir, '*.html'))
              for html_file in html_files:
                  if 'index.html' in html_file or 'search.html' in html_file:
                      continue
                  try:
                      output_path = html_to_markdown(html_file, output_dir)
                      print(f"Converted {html_file} to {output_path}")
                  except Exception as e:
                      print(f"Error processing {html_file}: {e}")
              
              # Copy existing markdown files
              md_files = glob.glob('docs/*.md')
              for md_file in md_files:
                  filename = os.path.basename(md_file)
                  output_path = os.path.join(output_dir, filename)
                  with open(md_file, 'r', encoding='utf-8') as f_in:
                      with open(output_path, 'w', encoding='utf-8') as f_out:
                          f_out.write(f_in.read())
                  print(f"Copied {md_file} to {output_path}")
              
              # Copy Sphinx-generated HTML files and convert to markdown
              sphinx_html_dir = 'docs/_build/html'
              if os.path.exists(sphinx_html_dir):
                  sphinx_files = glob.glob(os.path.join(sphinx_html_dir, '*.html'))
                  for html_file in sphinx_files:
                      if 'index.html' in html_file or 'search.html' in html_file:
                          continue
                      
                      # Convert the HTML filename to match original MD filename if possible
                      filename = os.path.basename(html_file).replace('.html', '.md')
                      # Skip files that would overwrite existing ones
                      if os.path.exists(os.path.join(output_dir, filename)):
                          continue
                          
                      try:
                          output_path = html_to_markdown(html_file, output_dir)
                          print(f"Converted Sphinx HTML {html_file} to {output_path}")
                      except Exception as e:
                          print(f"Error processing Sphinx HTML {html_file}: {e}")
              
              # Create Home.md (wiki landing page)
              with open(os.path.join(output_dir, 'Home.md'), 'w', encoding='utf-8') as f:
                  f.write("# SimOS NV Simulator Documentation\n\n")
                  f.write("Welcome to the documentation for the SimOS NV Simulator, a comprehensive quantum simulator for NV centers in diamond.\n\n")
                  f.write("*Developed by Leon Kaiser*\n\n")
                  f.write("This documentation is automatically generated from the source code using both Doxygen and Sphinx.\n\n")
                  f.write("## Contents\n\n")
                  
                  # Add links to key files
                  f.write("### Core Documentation\n\n")
                  f.write("- [Physical Model](physical_model.md) - Detailed explanation of NV center physics\n")
                  f.write("- [API Reference](api_reference.md) - Sphinx-generated API documentation\n")
                  f.write("- [Class Reference](class_physical_n_v_model.md) - Doxygen-generated class documentation\n\n")
                  
                  f.write("### Technical Implementation\n\n")
                  f.write("- [Quantum Hamiltonian](class_physical_n_v_model.md#_update_hamiltonian) - Full Hamiltonian implementation with strain and hyperfine effects\n")
                  f.write("- [Decoherence Modeling](class_physical_n_v_model.md#_get_c_ops) - Lindblad collapse operators for T1/T2 processes\n")
                  f.write("- [Adaptive Timestep Algorithm](class_physical_n_v_model.md#_get_optimal_timestep) - Efficient time evolution with error control\n")
                  f.write("- [Error Handling](class_physical_n_v_model.md#evolve) - Robust exception handling for quantum simulations\n\n")
                  
                  f.write("### Physics Background\n\n")
                  f.write("- [Zero-Field Splitting](physical_model.md#zero-field-splitting) - The D term in the NV Hamiltonian\n")
                  f.write("- [Zeeman Effect](physical_model.md#zeeman-interaction) - Magnetic field interaction\n")
                  f.write("- [Strain Effects](physical_model.md#strain-effects) - Crystal strain influence on energy levels\n")
                  f.write("- [Hyperfine Coupling](physical_model.md#hyperfine-interaction) - Electron-nuclear spin interactions\n")
                  f.write("- [Quantum Decoherence](physical_model.md#quantum-decoherence) - T1 and T2 relaxation processes\n")
          
          if __name__ == "__main__":
              main()
          EOF
          
          python convert.py
          
      - name: Setup Git for Wiki
        run: |
          git config --global user.name "Leon Kaiser"
          git config --global user.email "leon.kaiser@example.com"

      - name: Check if Wiki exists and clone it
        id: check-wiki
        run: |
          # Versuche, das Wiki-Repository zu klonen
          REPO_SLUG=$(echo $GITHUB_REPOSITORY | tr '[:upper:]' '[:lower:]')
          if git clone "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO_SLUG}.wiki.git" wiki-repo; then
            echo "Wiki exists and was cloned successfully"
            echo "wiki_exists=true" >> $GITHUB_OUTPUT
          else
            echo "Wiki does not exist or couldn't be cloned"
            echo "wiki_exists=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Create Wiki if it doesn't exist
        if: steps.check-wiki.outputs.wiki_exists == 'false'
        run: |
          echo "::warning::Wiki repository doesn't exist yet. Please create the wiki manually first."
          echo "To create the wiki:"
          echo "1. Go to your repository on GitHub: https://github.com/$GITHUB_REPOSITORY"
          echo "2. Click on the 'Wiki' tab"
          echo "3. Click 'Create the first page'"
          echo "4. Add some content (e.g., 'Initial wiki page')"
          echo "5. Click 'Save Page'"
          echo "6. Then re-run this workflow"
          
          # Create a directory for the output to avoid errors in later steps
          mkdir -p wiki-repo
          exit 0
      
      - name: Copy documentation to wiki
        if: steps.check-wiki.outputs.wiki_exists == 'true'
        run: |
          # Copy generated documentation to wiki repo
          cp -r wiki/* wiki-repo/
          cd wiki-repo
          
          # Check for changes
          if git status --porcelain | grep .; then
            git add .
            git commit -m "Update documentation from commit ${{ github.sha }} by Leon Kaiser"
            
            echo "Pushing to wiki repository..."
            
            # Wiki repos use 'master' branch by default
            git push origin master
            
            echo "Documentation successfully updated in the wiki!"
          else
            echo "No changes to commit"
          fi
          
      - name: Upload documentation as artifact (fallback)
        uses: actions/upload-artifact@v3
        with:
          name: wiki-documentation
          path: wiki/
          retention-days: 5